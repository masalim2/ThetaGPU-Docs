{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Data Science Software Availability On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time. Containers As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Home"},{"location":"#data-science-software-availability","text":"On Theta GPU, currently we support the major deep learning frameworks through two paths: singularity containers, based off of Nvidia's docker containers, and through bare-metal source builds. The bare-metal builds are so far only for tensorflow 2.X, with plans to support pytorch soon. Tensorflow 1.X is supported only via Nvidia's containers at this time.","title":"Data Science Software Availability"},{"location":"#containers","text":"As of now, the nvidia containers with tensorflow 1, 2 and pytorch built against cuda11, cudnn8 are available in singularity format here: $ ls /lus/theta-fs0/projects/datascience/thetaGPU/containers/ pytorch_20.08-py3.sif tf1_20.08-py3.sif tf2_20.08-py3.sif Execute a container interactively like this: $ singularity exec --nv -B /lus:/lus /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf1_20.08-py3.sif bash","title":"Containers"},{"location":"GPU%20Monitoring/","text":"GPU Monitoring Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example. GPU Selection In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-monitoring","text":"Each GPU on ThetaGPU hosts 8 A100 GPUs. You can see information about these GPUs via the command nvidia-smi . Each GPU has 40Gb of on-GPU memory. When you run applications, you will know the GPU is in use when you see the memory increase and the GPU Utilization will be non-zero. You can target a specific GPU with nvidia-smi -i 0 for the first GPU, for example.","title":"GPU Monitoring"},{"location":"GPU%20Monitoring/#gpu-selection","text":"In many application codes, you may want to specifiy which GPU is used. This is particular important in node-sharing applications where each GPU is running it's own code, which can be either in data-parallel model training, workflow based throughput jobs, etc. You can control individual process launches with: # Specify to run only on GPU 4: export CUDA_VISIBLE_DEVICES = 4 # Let your application see GPUS 0, 1, and 7: export CUDA_VISIBLE_DEVICES = \"0,1,7\" In these cases, the GPU orderings will appear as a consecutive list starting with 0. From inside an application, many software frameworks have ability to let you target specific GPUs, including tensorflow and pytorch: Tensorflow Pytorch","title":"GPU Selection"},{"location":"Singularity%20Containers/","text":"Nvidia Containers Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Singularity Containers"},{"location":"Singularity%20Containers/#nvidia-containers","text":"Nvidia delivers docker containers that contain their latest release of CUDA, tensorflow, pytorch, etc. You can see the full support matrix for all of their containers here: Nvidia support matrix Docker is not runnable on ALCF's ThetaGPU system for most users, but singularity is. To convert one of these images to singularity you can use the following command: singularity build $OUTPUT_NAME $NVIDIA_CONTAINER_LOCATION where $OUTPUT_NAME is typically of the form tf2_20.09-py3.simg and $NVIDIA_CONTAINER_LOCATION can be a docker url such as docker://nvcr.io/nvidia/tensorflow:20.09-tf2-py3 You can find the latest containers from Nvidia here: - Tensorflow 1 and 2 - Pytorch For your convienience, we've converted these containers to singularity and are available here: /lus/theta-fs0/software/thetagpu/nvidia-containers/ To extend the python libraries in these containers, please see building python packages . For running with these containers, please see Nvidia container notes . For issues with these containers, please email support@alcf.anl.gov .","title":"Nvidia Containers"},{"location":"building_python_packages/","text":"To build python packages for Theta GPU, there are two options: build on top of a bare-metal build (currently not available, but coming soon) or build on top of (and within) a singularity container. Additionally, you can build a new container from nvidia's docker images. Building on top of a container At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages. Reaching the outside world for pip packages You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export http_proxy = http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py Building custom packages Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful. HDF5 You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success! Horovod Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Building Python Packages"},{"location":"building_python_packages/#building-on-top-of-a-container","text":"At the moment, you will need two shells to do this: have one open on a login node (for example, thetaloginN , and one open on a compute node ( thetagpuN ). First, start the container in interactive mode: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/pytorch_20.08-py3.sif bash From here, you can create a virtual env for installation: export VENV_LOCATION = /path/to/virtualenv # replace this with your path! python -m venv --system-site-packages $VENV_LOCATION Note: sometimes, the venv package is available and if not, you can try python -m virtualenv . If neither are available, you can install it in your user directory: pip install --user virtualenv and it should work. Next time you log in, you'll have to start the container, and then run source $VENV_LOCATION/bin/activate to re-enable your installed packages.","title":"Building on top of a container"},{"location":"building_python_packages/#reaching-the-outside-world-for-pip-packages","text":"You'll notice right away when you try to pip install you can not, because the connection fails. You can, however, go through a proxy server for pip by enabling these variables: export http_proxy = http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy = https://theta-proxy.tmi.alcf.anl.gov:3128 Now, you can pip install your favorite packages: pip install mpi4py","title":"Reaching the outside world for pip packages"},{"location":"building_python_packages/#building-custom-packages","text":"Most packages (hdf5, for example, or python packages) can be built and installed into your virtual env. Here are two common examples that aren't currently part of the pytorch container that may be useful.","title":"Building custom packages"},{"location":"building_python_packages/#hdf5","text":"You can find the source code for hdf5 on their website https://www.hdfgroup.org/downloads/hdf5/source-code/. When downloaded and un-tarred, cd to the directory and run ./configure --prefix = $VENV_LOCATION # Add any other configuration arguments make -j 64 make install This should get you hdf5! For example, after this: (pytorch_20.08) Singularity> which h5cc /home/cadams/ThetaGPU/venvs/pytorch_20.08/bin/h5cc # This is my virtualenv, success!","title":"HDF5"},{"location":"building_python_packages/#horovod","text":"Horovod is useful for distributed training. To use it, you need it enabled within the container. git clone https://github.com/horovod/horovod.git cd horovod git submodule update --init python setup.py build python setup.py install This should install horovod within your container.","title":"Horovod"},{"location":"data_parallel_training/","text":"Distributed training on ThetaGPU using data parallelism Author: Huihuo Zheng huihuo.zheng@anl.gov There are two schemes for distributed learning: Model parallelization : in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f Data parallelization : in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM In this documentation, we would like to show how to do data parallel training on ThetaGPU. I. Software environement setup We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh II. TensorFlow with Horovod 1) Initialize Horovod import horovod.tensorflow as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank gpus = tf . config . experimental . list_physical_devices ( 'GPU' ) for gpu in gpus : tf . config . experimental . set_memory_growth ( gpu , True ) if gpus : tf . config . experimental . set_visible_devices ( gpus [ hvd . local_rank ()], 'GPU' ) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). opt = tf . train . AdagradOptimizer ( 0.01 * hvd . size ()) 4) Wrap the optimizer with Distributed Optimizer opt = hvd . DistributedOptimizer ( opt ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hooks = [hvd.BroadcastGlobalVariablesHook(0)] 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py III. PyTorch with Horovod It is very similar for PyTorch with Horovod 1) Initialize Horovod import horovod.torch as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank torch . cuda . set_device ( hvd . local_rank ()) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). optimizer = optim . SGD ( model . parameters (), lr = args . lr * hvd . size (), momentum = args . momentum ) 4) Wrap the optimizer with Distributed Optimizer optimizer = hvd . DistributedOptimizer ( optimizer , named_parameters = model . named_parameters (), compression = compression ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) 8) Average metric across all the workers Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average. def tensor_average ( val , name ): tensor = torch . tensor ( val ) if ( with_hvd ): avg_tensor = hvd . allreduce ( tensor , name = name ) else : avg_tensor = tensor return avg_tensor . item () We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/pytorch_mnist.py IV. PyTorch with DDP PyTorch has its own native parallelization library called DDP. We will provide omre details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html. V. MPI Profiling for data parallel training We support two ways for profling the performance of data parallel training. 1) mpitrace library MPI trace allows us to get a flat profling of all the MPI function calls involved during the training. To enable this, one can set the environment variable export LD_PRELOAD = /lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so Then run the application as usual. MPI profiling results will be generated after the run finishes mpi_profile.XXXX.[rank_id]. Below is an example output Data for MPI rank 0 of 8: Times and statistics from MPI_Init() to MPI_Finalize(). ----------------------------------------------------------------------- MPI Routine #calls avg. bytes time(sec) ----------------------------------------------------------------------- MPI_Comm_rank 3 0.0 0.000 MPI_Comm_size 3 0.0 0.000 MPI_Bcast 520 197140.6 0.518 MPI_Allreduce 24561 208138.3 162.080 MPI_Gather 126 4.0 0.363 MPI_Gatherv 126 0.0 0.434 MPI_Allgather 2 4.0 0.000 ----------------------------------------------------------------- MPI task 0 of 8 had the maximum communication time. total communication time = 163.396 seconds. total elapsed time = 187.298 seconds. user cpu time = 4127.728 seconds. system time = 728.100 seconds. max resident set size = 8403.938 MBytes. Rank 0 reported the largest memory utilization : 8403.94 MBytes Rank 0 reported the largest elapsed time : 187.30 sec ----------------------------------------------------------------- Message size distributions: MPI_Bcast #calls avg. bytes time(sec) 126 4.0 0.008 1 8.0 0.000 121 25.0 0.006 30 251.5 0.002 32 512.0 0.002 64 1024.0 0.005 44 2048.0 0.003 29 4092.8 0.003 16 8192.0 0.032 MPI_Allreduce #calls avg. bytes time(sec) 19780 8.0 90.822 4576 24.0 18.239 43 4004.0 0.295 5 2780979.2 0.469 50 8160289.2 20.893 9 11803392.0 0.964 48 28060640.0 3.293 50 64731668.5 27.105 MPI_Gather #calls avg. bytes time(sec) 126 4.0 0.363 The useful information here is the message size distribution. 2) Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser. More details: https://horovod.readthedocs.io/en/stable/timeline_include.html","title":"Data Parallel Training"},{"location":"data_parallel_training/#distributed-training-on-thetagpu-using-data-parallelism","text":"Author: Huihuo Zheng huihuo.zheng@anl.gov There are two schemes for distributed learning: Model parallelization : in this scheme, disjoint subsets of a neural network are assigned to different devices. Therefore, all the computations associated to the subsets are distributed. Communication happens between devices whenever there is dataflow between two subsets. Model parallelization is suitable when the model is too large to be fitted into a single device (CPU/GPU) because of the memory capacity. However, partitioning the model into different subsets is not an easy task, and there might potentially introduce load imbalance issues limiting the scaling efficiency.\u202f Data parallelization : in this scheme, all the workers own a replica of the model. The global batch of data is split into multiple minibatches,\u202fand processed by different workers. Each worker computes the corresponding loss and gradients with respect to the data it posseses. Before the updating of the parameters at each epoch, the loss and gradients are averaged among all the workers through a collective operation. This scheme is relatively simple to implement. MPI_Allreduce is the only commu Our recent presentation about the data parallel training can be found here: https://youtu.be/930yrXjNkgM In this documentation, we would like to show how to do data parallel training on ThetaGPU.","title":"Distributed training on ThetaGPU using data parallelism"},{"location":"data_parallel_training/#i-software-environement-setup","text":"We are still in the process of setting up the software stacks on ThetaGPU. Currently, one can get TensorFlow, PyTorch, and Horovod with the following setup script. source /lus/theta-fs0/software/datascience/thetagpu/anaconda3/setup.sh","title":"I. Software environement setup"},{"location":"data_parallel_training/#ii-tensorflow-with-horovod","text":"1) Initialize Horovod import horovod.tensorflow as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank gpus = tf . config . experimental . list_physical_devices ( 'GPU' ) for gpu in gpus : tf . config . experimental . set_memory_growth ( gpu , True ) if gpus : tf . config . experimental . set_visible_devices ( gpus [ hvd . local_rank ()], 'GPU' ) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). opt = tf . train . AdagradOptimizer ( 0.01 * hvd . size ()) 4) Wrap the optimizer with Distributed Optimizer opt = hvd . DistributedOptimizer ( opt ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hooks = [hvd.BroadcastGlobalVariablesHook(0)] 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/tensorflow2_mnist.py","title":"II. TensorFlow with Horovod"},{"location":"data_parallel_training/#iii-pytorch-with-horovod","text":"It is very similar for PyTorch with Horovod 1) Initialize Horovod import horovod.torch as hvd hvd . init () After this initialization, the rank ID and the number of processes can be refered as hvd.rank() and hvd.size() . Besides, one can also call hvd.local_rank() to get the local rank ID within a node. This is useful when we are trying to assign GPUs to each rank. 2) Assign GPU to each rank torch . cuda . set_device ( hvd . local_rank ()) In this case, we set one GPU per process: ID= hvd.local_rank() 3) Scale the learning rate. Typically, since we use multiple workers, the global batch is usually increases n times (n is the number of workers). The learning rate should increase proportionally as follows (assuming that the learning rate initially is 0.01). optimizer = optim . SGD ( model . parameters (), lr = args . lr * hvd . size (), momentum = args . momentum ) 4) Wrap the optimizer with Distributed Optimizer optimizer = hvd . DistributedOptimizer ( optimizer , named_parameters = model . named_parameters (), compression = compression ) 5) Broadcast the model from rank 0 This is to make sure that all the workers will have the same starting point. hvd.broadcast_parameters(model.state_dict(), root_rank=0) hvd.broadcast_optimizer_state(optimizer, root_rank=0) 6) Loading data according to rank ID TensorFlow has some functions for parallel distribution of data. But for specific applications, the user might have to write their own data loader. In general, one has two ways to deal with the data loading. 1. Each worker randomly select one batch of data from the dataset at each step. In such case, each worker can see the entire dataset. It is important to make sure that the different worker have different random seeds so that they will get different data at each step. 2. Each worker accesses a subset of dataset. One manually partition the entire dataset into different partions, and each rank access one of the partions. In both cases, the total number of steps per epoch is nsamples / hvd.size() . 7) Checkpointing on root rank It is important to let only one process to do the checkpointing I/O lest perhaps the file been corrupted. if hvd . rank () == 0 : checkpoint . save ( checkpoint_dir ) 8) Average metric across all the workers Notice that in the distributed training, any tensor are local to each worker. In order to get the global averaged value, one can use Horovod allreduce. Below is an example on how to do the average. def tensor_average ( val , name ): tensor = torch . tensor ( val ) if ( with_hvd ): avg_tensor = hvd . allreduce ( tensor , name = name ) else : avg_tensor = tensor return avg_tensor . item () We provided some examples in: https://github.com/argonne-lcf/sdl_ai_workshop/blob/master/01_distributedDeepLearning/Horovod/pytorch_mnist.py","title":"III. PyTorch with Horovod"},{"location":"data_parallel_training/#iv-pytorch-with-ddp","text":"PyTorch has its own native parallelization library called DDP. We will provide omre details on how to run this on ThetaGPU. The current PyTorch on ThetaGPU does not have DDP built in. We will update to our users once we have DDP. For now, please refer to https://pytorch.org/tutorials/intermediate/ddp_tutorial.html.","title":"IV. PyTorch with DDP"},{"location":"data_parallel_training/#v-mpi-profiling-for-data-parallel-training","text":"We support two ways for profling the performance of data parallel training. 1) mpitrace library MPI trace allows us to get a flat profling of all the MPI function calls involved during the training. To enable this, one can set the environment variable export LD_PRELOAD = /lus/theta-fs0/software/datascience/thetagpu/hpctw/lib/libmpitrace.so Then run the application as usual. MPI profiling results will be generated after the run finishes mpi_profile.XXXX.[rank_id]. Below is an example output Data for MPI rank 0 of 8: Times and statistics from MPI_Init() to MPI_Finalize(). ----------------------------------------------------------------------- MPI Routine #calls avg. bytes time(sec) ----------------------------------------------------------------------- MPI_Comm_rank 3 0.0 0.000 MPI_Comm_size 3 0.0 0.000 MPI_Bcast 520 197140.6 0.518 MPI_Allreduce 24561 208138.3 162.080 MPI_Gather 126 4.0 0.363 MPI_Gatherv 126 0.0 0.434 MPI_Allgather 2 4.0 0.000 ----------------------------------------------------------------- MPI task 0 of 8 had the maximum communication time. total communication time = 163.396 seconds. total elapsed time = 187.298 seconds. user cpu time = 4127.728 seconds. system time = 728.100 seconds. max resident set size = 8403.938 MBytes. Rank 0 reported the largest memory utilization : 8403.94 MBytes Rank 0 reported the largest elapsed time : 187.30 sec ----------------------------------------------------------------- Message size distributions: MPI_Bcast #calls avg. bytes time(sec) 126 4.0 0.008 1 8.0 0.000 121 25.0 0.006 30 251.5 0.002 32 512.0 0.002 64 1024.0 0.005 44 2048.0 0.003 29 4092.8 0.003 16 8192.0 0.032 MPI_Allreduce #calls avg. bytes time(sec) 19780 8.0 90.822 4576 24.0 18.239 43 4004.0 0.295 5 2780979.2 0.469 50 8160289.2 20.893 9 11803392.0 0.964 48 28060640.0 3.293 50 64731668.5 27.105 MPI_Gather #calls avg. bytes time(sec) 126 4.0 0.363 The useful information here is the message size distribution. 2) Horovod Timeline To perform Horovod timeline analysis, one has to set the environment variable HOROVOD_TIMELINE which specifies the file for the output. export HOROVOD_TIMELINE=timeline.json This file is only recorded on rank 0, but it contains information about activity of all workers. You can then open the timeline file using the chrome://tracing facility of the Chrome browser. More details: https://horovod.readthedocs.io/en/stable/timeline_include.html","title":"V. MPI Profiling for data parallel training"},{"location":"gpu_node_queue_and_policy/","text":"GPU Node Queue and Policy Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary. Nodes vs Queue vs MIG mode The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script. Queues There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 12 hours MaxRunning will be 2 full nodes or 16 individual GPUs Queue Restrictions MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"GPU Node Queues and Policy"},{"location":"gpu_node_queue_and_policy/#gpu-node-queue-and-policy","text":"Note: Users will need an allocation on ThetaGPU to utilize the GPU nodes. Request for an allocation by filling out this form: Allocation Request . ThetaGPU is listed under Theta on the form. The GPU nodes are new and we expect the workload to be significantly different than it is on the KNL nodes. This document describes the current state of affairs, but we will monitor usage and adjust the policies as necessary.","title":"GPU Node Queue and Policy"},{"location":"gpu_node_queue_and_policy/#nodes-vs-queue-vs-mig-mode","text":"The GPU nodes are NVidia DGX A100 nodes and each node contains eight (8) A100 GPUs . You may request either entire nodes, or individual GPUs based on your job needs. What you will get is determined by the queue you submit to: If it has node in the name, you will get nodes. If it has GPU in the name, you will get GPUs. Note that the -n parameter in your qsub will match the resource type in the queue ( -n 2 in node queue will get you two full nodes, -n 2 in a GPU queue will get you two GPUs). Additionally, the Nvidia A100 GPUs support a feature called \u201cMulti-Instance GPU\u201d (MIG) mode . This allows a single GPU to be shared by up to 7 different processes. We do not schedule at this level, but you may pass \u2013attrs mig-mode=True in with your qsub and we will set the node to MIG mode and you may take advantage of it in your job script.","title":"Nodes vs Queue vs MIG mode"},{"location":"gpu_node_queue_and_policy/#queues","text":"There will be two primary queues: full-node : This is the general production queue for jobs that require full nodes. single-gpu : This is the general production queue for jobs that operate best on individual GPUs. And two debug queues: debug-node : Submit to this queue if you need an entire node for your testing (for instance you are utilizing the NVLink) debug-gpu : Submit to this queue if you need GPUs. Initially, we are relaxing our node restrictions to encourage early users. Please be courteous to your fellow users and do not monopolize the machine. We will tighten restrictions as required to manage the demonstrated workload. Here are the initial queue limits: MinTime is 5 minutes MaxTime is 12 hours MaxRunning will be 2 full nodes or 16 individual GPUs","title":"Queues"},{"location":"gpu_node_queue_and_policy/#queue-restrictions","text":"MaxQueued will be 100 jobs You may have at most 1152 node-hours or 9216 GPU hours in the queue at any time. You may not violate either of these policies. You could not submit (1000) 1 node-hour jobs because that would violate the MaxQueued of 100 jobs, nor could you submit (2) 1000 node-hour jobs because that would violate the MaxNodeHours limit. The initial queue policy will be simple First-In-First-Out (FIFO) based on priority with EASY backfill.","title":"Queue Restrictions"},{"location":"jupyter/","text":"Jupyter Instructions From a thetalogin node: ssh thetagpusn1 to login to a thetaGPU login node. From thetagpusn1 , start an interactive job (make sure to note which thetaGPU node the job gets routed to, thetagpu21 in this example): ( thetagpusn1 ) $ qsub -I -A datascience -n 1 -t 01 :00 -O interactive --attrs = pubnet = true Job routed to queue \"full-node\" . Wait for job 10003623 to start... Opening interactive session to thetagpu21 From the thetaGPU compute node, start a jupyter notebook: Note: This assumes you're in a suitable python environment containing jupyter , for more information on setting up a conda environment, see Running Tensorflow with Conda ): ( thetagpu21 ) $ jupyter notebook & From a new terminal (on your local machine): $ export PORT_NUM = 8889 # any number besides 8888 (the default) should work $ ssh -L $PORT_NUM :localhost:8888 username@theta.alcf.anl.gov ( thetalogin ) $ ssh -L 8888 :localhost:8888 thetagpusn1 ( thetagpusn1 ) $ ssh -L 8888 :localhost:8888 thetagpu21 Navigating to localhost:8889 (or whatever port number you chose above) on your local machine should then establish a connection to the jupyter backend!","title":"Jupyter"},{"location":"jupyter/#jupyter-instructions","text":"From a thetalogin node: ssh thetagpusn1 to login to a thetaGPU login node. From thetagpusn1 , start an interactive job (make sure to note which thetaGPU node the job gets routed to, thetagpu21 in this example): ( thetagpusn1 ) $ qsub -I -A datascience -n 1 -t 01 :00 -O interactive --attrs = pubnet = true Job routed to queue \"full-node\" . Wait for job 10003623 to start... Opening interactive session to thetagpu21 From the thetaGPU compute node, start a jupyter notebook: Note: This assumes you're in a suitable python environment containing jupyter , for more information on setting up a conda environment, see Running Tensorflow with Conda ): ( thetagpu21 ) $ jupyter notebook & From a new terminal (on your local machine): $ export PORT_NUM = 8889 # any number besides 8888 (the default) should work $ ssh -L $PORT_NUM :localhost:8888 username@theta.alcf.anl.gov ( thetalogin ) $ ssh -L 8888 :localhost:8888 thetagpusn1 ( thetagpusn1 ) $ ssh -L 8888 :localhost:8888 thetagpu21 Navigating to localhost:8889 (or whatever port number you chose above) on your local machine should then establish a connection to the jupyter backend!","title":"Jupyter Instructions"},{"location":"mpi/","text":"Launching a Singularity container with MPI #!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"MPI"},{"location":"mpi/#launching-a-singularity-container-with-mpi","text":"#!/bin/bash SINGULARITYBIN=$(which singularity) CONTAINER=${HOME}/singularity_images/hpl-mofed5-cuda11runtime_verbs.sif OMPI_MCA_orte_launch_agent=$(cat <<EOF $SINGULARITYBIN run --nv $CONTAINER orted EOF ) export SINGULARITYENV_OMPI_MCA_orte_launch_agent=${OMPI_MCA_orte_launch_agent} $SINGULARITYBIN run --nv --cleanenv \\ $CONTAINER \\ mpirun \\ -H thetagpu01:1,thetagpu02:1 \\ -n 2 \\ --mca plm_rsh_args \"-p22\" \\ hostname","title":"Launching a Singularity container with MPI"},{"location":"system_overview_and_node_information/","text":"ThetaGPU Machine Overview ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect. Login Nodes The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( ssh thetagpusn1 or ssh thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"System Overview and Node Information"},{"location":"system_overview_and_node_information/#thetagpu","text":"","title":"ThetaGPU"},{"location":"system_overview_and_node_information/#machine-overview","text":"ThetaGPU is an extension of Theta and is comprised of 24 NVIDIA DGX A100 nodes. Each DGX A100 node comprises eight NVIDIA A100 Tensor Core GPUs and two AMD Rome CPUs that provide 320 gigabytes (7680 GB aggregately) of GPU memory for training artificial intelligence (AI) datasets, while also enabling GPU-specific and -enhanced high-performance computing (HPC) applications for modeling and simulation. The DGX A100\u2019s integration into Theta is achieved via the ALCF\u2019s Cobalt HPC scheduler and shared access to a 10-petabyte Lustre filesystem. Fixed ALCF user accounts ensure a smooth onboarding process for the expanded system. A 15-terabyte solid-state drive offers up to 25 gigabits per second in bandwidth. The dedicated compute fabric comprises 20 Mellanox QM9700 HDR200 40-port switches wired in a fat-tree topology. ThetaGPU cannot utilize the Aries interconnect.","title":"Machine Overview"},{"location":"system_overview_and_node_information/#login-nodes","text":"The Theta login nodes will be the intended method to access ThetaGPU. At first, Cobalt jobs cannot be submitted from the theta login nodes to run on the GPU nodes; until that is supported, users will need to login to the ThetaGPU service nodes ( ssh thetagpusn1 or ssh thetagpusn2 ) from the Theta login nodes, from there, Cobalt jobs can be submitted to run on the GPU nodes. Table 1: Theta Machine Overview ThetaGPU Machine Specs Architecture NVIDIA DGX A100 Speed 3.9 petaflops Processors AMD EPYC 7742 Nodes 24 DDR4 Memory 24 TB GPU Memory 7,680 GB Racks 7 Table 2: ThetaGPU Compute Nodes Overview COMPONENT PER NODE AGGREGATE AMD Rome 64-core CPU 2 48 GPU Memory 320 GB 7,680 GB DDR4 Memory 1 TB 24 TB NVIDIA A100 GPU 8 192 HDR200 Compute Ports 8 192 HDR200 Storage Ports 2 48 100GbE Ports 2 48 3.84 TB Gen4 NVME drives 4 96","title":"Login Nodes"},{"location":"Building_Compiling/","text":"Description These are the steps to build code that has Python/C++ code interoperability. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: Activate the TensorFlow 2.2 Singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy=https://theta-proxy.tmi.alcf.anl.gov:3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python -m pip install --user virtualenv export VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here python -m virtualenv --system-site-packages $VENV_LOCATION source $VENV_LOCATION/bin/activate python -m pip install cmake python -m pip install matplotlib python -m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Python/C++ code interoperability"},{"location":"Building_Compiling/#description","text":"These are the steps to build code that has Python/C++ code interoperability. Login to a ThetaGPU head node ssh thetagpusn1 Request an interactive session on an A100 GPU qsub -n 1 -q default -A datascience -I -t 1:00:00 Following this, we need to execute a few commands to get setup with an appropriately optimized TensorFlow. These are: Activate the TensorFlow 2.2 Singularity container: singularity exec -B /lus:/lus --nv /lus/theta-fs0/projects/datascience/thetaGPU/containers/tf2_20.08-py3.sif bash Setup access to the internet export http_proxy=http://theta-proxy.tmi.alcf.anl.gov:3128 export https_proxy=https://theta-proxy.tmi.alcf.anl.gov:3128 Now that we can access the internet, we need to set up a virtual environment in Python (these commands should only be run the first time) python -m pip install --user virtualenv export VENV_LOCATION=/home/rmaulik/THETAGPU_TF_ENV # Add your path here python -m virtualenv --system-site-packages $VENV_LOCATION source $VENV_LOCATION/bin/activate python -m pip install cmake python -m pip install matplotlib python -m pip install sklearn cmake is required to build our C++ app and link to Python, and other packages may be pip installed as needed in your Python code. An example CMakeLists.txt file for building with Python/C interoperability with examples can be found here .","title":"Description"},{"location":"Building_Compiling/compiling_and_linking/","text":"Compiling and Linking ThetaGPU basically has AMD processors on the service nodes ( thetagpusn1,2 ) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I ), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"Building_Compiling/compiling_and_linking/#compiling-and-linking","text":"ThetaGPU basically has AMD processors on the service nodes ( thetagpusn1,2 ) and AMD processors and NVIDIA A100 GPUs on the compute nodes [see overview page]. The service nodes can be used to create containers and launch jobs, and eventually to use as a cross-compiling environment for compute nodes. Until the cross-compiling environment is set up, the compute nodes will have to be used for compiling. This can be done by using an interactive Cobalt job (via qsub -I ), or until we have reserved or added a dedicated build node. The default programming environment on the ThetaGPU compute nodes is the GNU compiler tools coupled with NVIDIA\u2019s CUDA toolkit. For non-GPU codes: gcc for C compiler g++ for C++ gfortran for Fortran For CUDA codes: nvcc For MPI, the latest MPI is in /lus/theta-fs0/software/thetagpu/openmpi-4.0.5. mpicc mpicxx mpif77/mpif90 not configured yet mpirun is a wrapper in /usr/local/bin that sets the appropriate options and uses the mpirun in the MPI directory above. On the service nodes, GNU compilers are available. There are no modules/modulefiles yet set up on ThetaGPU.","title":"Compiling and Linking"},{"location":"daskmpi/","text":"Dask-mpi on Theta How to run Dask-MPI on Theta at Argonne Leadership Computing Facility. Install Submit a batch job Run a script in an interactive session Start a JupyterLab interactive session Install ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Load Miniconda module load miniconda-3 Create a new conda environment conda create --name envname --clone $CONDA_PREFIX Activate the environment source activate envname Install mpi4py from the alcf-theta channel conda install -c alcf-theta mpi4py Install the other dependencies from conda-forge conda install -c conda-forge dask dask-mpi bokeh jupyter Jupyterlab ipykernel ipyparallel pip Download the files start_daskmpi.py and dask_example.py from this repository Local storage If your dataset is larger than the combined memory of all compute nodes, Dask will spill excess data to disk. If you do not have write permission to local storage on the compute nodes, spilling to disk will be disabled by default, as explained here . See these instructions on how to request access to local storage on Theta's compute nodes. Submit a batch job This will run dask_example.py using 4 ranks on two nodes. ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit a batch job on n nodes qsub -n 2 -t 30 -A datascience -q debug-cache-quad daskmpi_job.sh where the script daskmpi_job.sh is source activate envname cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py dask_example.py Run a script in an interactive session ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit an interactive job on n nodes qsub -n 2 -t 30 -I --attrs enable_ssh = 1 -A datascience -q debug-cache-quad A shell opens up on one of the mom nodes Activate the environment source activate envname Run the example script. It is \" Exercise: Parallelize a for loop \" of the Dask tutorial on dask.delayed cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py dask_example.py The output should be similar to the following: Starting the scheduler Scheduler address: tcp://10.128.15.17:8786 on node nid03826 Starting the workers Client status: <Client: 'tcp://10.128.15.17:8786' processes=3 threads=3, memory=608.02 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetamom1 ssh -t -L 8787:localhost:8787 nid03826 To open the Dask dashboard, go to: http://localhost:8787/status 44 elapsed time: 8.008632 44 elapsed time: 3.182807 Code ran successfully. Successfully exited Dask dashboard You can connect to the Dask dashboard on http://localhost:8787/status in you browser after you run the ssh command printed in the above output message in a shell on your local machine. Start a JupyterLab interactive session ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit an interactive job on n nodes qsub -n 2 -t 30 -I --attrs enable_ssh = 1 -A datascience -q debug-cache-quad A shell opens up on one of the mom nodes Activate the environment source activate envname Run the start_daskmpi.py script without any argument to start a JupyterLab session cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py Type stop to terminate the Dask session. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://10.128.15.25:8786 on node nid03834 Starting the workers Client status: <Client: 'tcp://10.128.15.25:8786' processes=3 threads=3, memory=608.02 GB> Starting JupyterLab on the scheduler... To connect to JupyterLab and Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetamom1 ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 nid03834 To open JupyterLab, go to (see log file /home/username/dask_logs/jupyterlab.log): http://localhost:7787/?token=39212debfed9bb19c78bf3d87f4a3b8d75bc2cce9087724e To open the Dask dashboard, go to: http://localhost:8787/status JupyterLab started. Type 'stop' to stop Dask: stop Successfully exited JupyterLab and Dask dashboard You can connect to JupyterLab on http://localhost:7787/ in you browser and view the Dask dashboard on http://localhost:8787/status after you run the ssh command printed in the above output message in a shell on your local machine.","title":"Dask-MPI"},{"location":"daskmpi/#dask-mpi-on-theta","text":"How to run Dask-MPI on Theta at Argonne Leadership Computing Facility. Install Submit a batch job Run a script in an interactive session Start a JupyterLab interactive session","title":"Dask-mpi on Theta"},{"location":"daskmpi/#install","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Load Miniconda module load miniconda-3 Create a new conda environment conda create --name envname --clone $CONDA_PREFIX Activate the environment source activate envname Install mpi4py from the alcf-theta channel conda install -c alcf-theta mpi4py Install the other dependencies from conda-forge conda install -c conda-forge dask dask-mpi bokeh jupyter Jupyterlab ipykernel ipyparallel pip Download the files start_daskmpi.py and dask_example.py from this repository","title":"Install"},{"location":"daskmpi/#local-storage","text":"If your dataset is larger than the combined memory of all compute nodes, Dask will spill excess data to disk. If you do not have write permission to local storage on the compute nodes, spilling to disk will be disabled by default, as explained here . See these instructions on how to request access to local storage on Theta's compute nodes.","title":"Local storage"},{"location":"daskmpi/#submit-a-batch-job","text":"This will run dask_example.py using 4 ranks on two nodes. ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit a batch job on n nodes qsub -n 2 -t 30 -A datascience -q debug-cache-quad daskmpi_job.sh where the script daskmpi_job.sh is source activate envname cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py dask_example.py","title":"Submit a batch job"},{"location":"daskmpi/#run-a-script-in-an-interactive-session","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit an interactive job on n nodes qsub -n 2 -t 30 -I --attrs enable_ssh = 1 -A datascience -q debug-cache-quad A shell opens up on one of the mom nodes Activate the environment source activate envname Run the example script. It is \" Exercise: Parallelize a for loop \" of the Dask tutorial on dask.delayed cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py dask_example.py The output should be similar to the following: Starting the scheduler Scheduler address: tcp://10.128.15.17:8786 on node nid03826 Starting the workers Client status: <Client: 'tcp://10.128.15.17:8786' processes=3 threads=3, memory=608.02 GB> To connect to the Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 8787:localhost:8787 thetamom1 ssh -t -L 8787:localhost:8787 nid03826 To open the Dask dashboard, go to: http://localhost:8787/status 44 elapsed time: 8.008632 44 elapsed time: 3.182807 Code ran successfully. Successfully exited","title":"Run a script in an interactive session"},{"location":"daskmpi/#dask-dashboard","text":"You can connect to the Dask dashboard on http://localhost:8787/status in you browser after you run the ssh command printed in the above output message in a shell on your local machine.","title":"Dask dashboard"},{"location":"daskmpi/#start-a-jupyterlab-interactive-session","text":"ssh into one of Theta's login nodes ssh username@theta.alcf.anl.gov Submit an interactive job on n nodes qsub -n 2 -t 30 -I --attrs enable_ssh = 1 -A datascience -q debug-cache-quad A shell opens up on one of the mom nodes Activate the environment source activate envname Run the start_daskmpi.py script without any argument to start a JupyterLab session cd dask_mpi/ aprun -n 4 -N 2 python start_daskmpi.py Type stop to terminate the Dask session. The output should be similar to the following: Starting the scheduler Scheduler address: tcp://10.128.15.25:8786 on node nid03834 Starting the workers Client status: <Client: 'tcp://10.128.15.25:8786' processes=3 threads=3, memory=608.02 GB> Starting JupyterLab on the scheduler... To connect to JupyterLab and Dask dashboard, execute the following command in a shell on your local machine: ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 username@theta.alcf.anl.gov ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 thetamom1 ssh -t -L 7787:localhost:7787 -L 8787:localhost:8787 nid03834 To open JupyterLab, go to (see log file /home/username/dask_logs/jupyterlab.log): http://localhost:7787/?token=39212debfed9bb19c78bf3d87f4a3b8d75bc2cce9087724e To open the Dask dashboard, go to: http://localhost:8787/status JupyterLab started. Type 'stop' to stop Dask: stop Successfully exited","title":"Start a JupyterLab interactive session"},{"location":"daskmpi/#jupyterlab-and-dask-dashboard","text":"You can connect to JupyterLab on http://localhost:7787/ in you browser and view the Dask dashboard on http://localhost:8787/status after you run the ssh command printed in the above output message in a shell on your local machine.","title":"JupyterLab and Dask dashboard"},{"location":"ml_frameworks/pytorch/running_with_conda/","text":"Running PyTorch with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. PyTorch (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of PyTorch which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/pt_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the PyTorch repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11-25 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/pt_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer PyTorch build. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Running with Conda"},{"location":"ml_frameworks/pytorch/running_with_conda/#running-pytorch-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs.","title":"Running PyTorch with Conda"},{"location":"ml_frameworks/pytorch/running_with_conda/#pytorch-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of PyTorch which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/pt_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the PyTorch repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11-25 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/pt_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer PyTorch build. This package will also include the latest Horovod tagged release.","title":"PyTorch (master build)"},{"location":"ml_frameworks/pytorch/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/pytorch/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/pytorch/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/","text":"NVidia Container Notes Getting the container To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/ Running on ThetaGPU After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different. Running Tensorflow-2 with Horovod on ThetaGPU To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running with Singularity"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#nvidia-container-notes","text":"","title":"NVidia Container Notes"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#getting-the-container","text":"To get NVidia docker containers which have the latest CUDA and Tensorflow installed, go to NVidia NGC , create an account, search for Tensorflow . Notice there are containers tagged with tf1 and tf2 . The page tells you how to select the right one. You can convert the command at the top, for instance: docker pull nvcr.io/nvidia/tensorflow:20.08-tf2-py3 to a singularity command by doing this: singularity build tensorflow-20.08-tf2-py3.simg docker://nvcr.io/nvidia/tensorflow:20.08-tf2-py3 You'll need to run this command on a Theta login node which has network access ( thetaloginX ). The containers from August, 2020, are also all available converted to singularity here: /lus/theta-fs0/projects/datascience/thetaGPU/containers/","title":"Getting the container"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-on-thetagpu","text":"After logging into ThetaGPU with ssh thetagpusn1 , one can submit job using the container one a single node by doing: qsub -n 1 -t 10 -A <project-name> submit.sh where submit.sh contians the following bash scripting: #!/bin/bash CONTAINER = $HOME /tensorflow-20.08-tf2-py3.simg singularity exec --nv $CONTAINER python /usr/local/lib/python3.6/dist-packages/tensorflow/python/debug/examples/debug_mnist.py make sure to make the script executable with chmod a+x submit.sh . The log file <cobalt-jobid>.output should contain some text like this: Accuracy at step 0 : 0 .2159 Accuracy at step 1 : 0 .098 Accuracy at step 2 : 0 .098 Accuracy at step 3 : 0 .098 Accuracy at step 4 : 0 .098 Accuracy at step 5 : 0 .098 Accuracy at step 6 : 0 .098 Accuracy at step 7 : 0 .098 Accuracy at step 8 : 0 .098 Accuracy at step 9 : 0 .098 The numbers may be different.","title":"Running on ThetaGPU"},{"location":"ml_frameworks/tensorflow/nvidia_container_notes/#running-tensorflow-2-with-horovod-on-thetagpu","text":"To run on ThetaGPU with MPI you can do the follow test: git clone git@github.com:jtchilders/tensorflow_skeleton.git cd tensorflow_skeleton qsub -n 2 -t 20 -A <project-name> submit_scripts/thetagpu_mnist.sh You can inspect the submit script for details on how the job is constructed.","title":"Running Tensorflow-2 with Horovod on ThetaGPU"},{"location":"ml_frameworks/tensorflow/running_with_conda/","text":"Running Tensorflow with Conda Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs. Tensorflow (master build) Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release. Installing Packages Using pip install --user With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages . Using Conda Environments If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Running with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#running-tensorflow-with-conda","text":"Beware that these builds use CUDA and will not work on login nodes, which does not have CUDA installed as there are no GPUs.","title":"Running Tensorflow with Conda"},{"location":"ml_frameworks/tensorflow/running_with_conda/#tensorflow-master-build","text":"Given A100 and CUDA 11 are very new, we have a build of the master branch of Tensorflow which includes better performance and support for these architectures. Users can utilize them by running this setup script: source /lus/theta-fs0/software/thetagpu/conda/tf_master/latest/mconda3/setup.sh This will setup a conda environment with a recent \"from scratch\" build of the Tensorflow repository on the master branch. The latest in the path is a symlink to a directory named by date that will be used to track our local builds. Per the writing of this documetation the only build uses latest points to 2020-11 . In the future, there will be newer builds available in that directory /lus/theta-fs0/software/thetagpu/conda/tf_master/ so check there for newer installs and run the respective mconda3/setup.sh script to use it. If you find things break since the last time you ran, it may be because latest is now pointing at a newer Tensorflow build. This package will also include the latest Horovod tagged release.","title":"Tensorflow (master build)"},{"location":"ml_frameworks/tensorflow/running_with_conda/#installing-packages","text":"","title":"Installing Packages"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-pip-install-user","text":"With the conda environment setup, one can install common Python modules using pip install --users <module-name> which will install packages in $HOME/.local/lib/pythonX.Y/site-packages .","title":"Using pip install --user"},{"location":"ml_frameworks/tensorflow/running_with_conda/#using-conda-environments","text":"If you need more flexibility, you can clone the conda environment into a custom path, which would then allow for root-like installations via conda install <module> or pip install <module> . First, setup the conda environment you want to use as instructed above. Second, clone the environment into a local path to which you have write access conda create --clone $CONDA_PREFIX -p <path/to/env> Then activate that environment: conda activate <path/to/env> One should then be able to install modules freely.","title":"Using Conda Environments"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/","text":"Tensorboard Instructions After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard"},{"location":"ml_frameworks/tensorflow/tensorboard_instructions/#tensorboard-instructions","text":"After you have logged into ThetaGPU, and have a Tensorflow run going, you'll need to know one of your worker nodes so you can SSH to it. PORT0 = 9991 PORT1 = 9992 PORT3 = 9993 # Select a theta login node N where N=[1-6] ssh -L $PORT0 :localhost: $PORT1 $USER @thetaloginN.alcf.anl.gov # after reaching thetaloginN # Replace NN with your thetagpu worker node ssh -L $PORT1 :thetagpuNN: $PORT3 $USER @thetagpusn1 # after reaching thetagpusn1 # login to worker node ssh thetagpuNN # now setup your tensorflow environment # for instance run the conda setup.sh script created during the install_tensorflow.sh script # now run tensorboard tensorboard --logdir </path/to/logs> --port $PORT3 --bind_all","title":"Tensorboard Instructions"}]}